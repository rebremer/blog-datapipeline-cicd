variables:
  # Azure DevOps settings
  AzureServiceConnectionId: 'blog-datapipelineprod-servcon'
  # Change environment variables used in bash scripts with your own
  RG: 'blog-datapipelineprod-rg'
  SUB: '<<your subscription>>'
  AKV: 'blogdatapipelineakv123' # unique value
  STOR: 'blogdatapipelinestor123' # unique value
  COSMOSDBNAME: 'blog-datapipeline-cosmos123' #unique value
  DBRWORKSPACE: 'blog-datapipeline-dbr123' #unique value
  # fixed Environment variables, no need for unique values
  LOC: 'westeurope'
  SPN: 'blog-datapipeline-spn'
  ADFV2: 'blog-datapipeline-adfv2'
  VNET: 'blog-datapipeline-vnet'
  # global settings
  ACCESS_STOR_AADDBR: 0
  SECRETSCOPE_KEYVAULT: 0 # see script 4_configure_secret_scope_databricks.sh in key vault is used
  MOUNT_STORAGE_DATABRICKS: 0 # can only be used if access_store_AAD is true
  ENABLE_FIREWALL: 1

trigger:
- master

pool:
  vmImage: 'ubuntu-latest'

resources:
  repositories:
  - repository: blog-datapipeline-cicd # change with your own repo name when necessary
    type: git
    name: blog-datapipeline-devops
    ref: master
  - repository: blog-datapipeline-deployadfv2 # change with your own repo name when necessary
    type: git
    name: blog-datapipeline-devops
    ref: adf_publish

steps:
- checkout: blog-datapipeline-cicd
  path: blog-datapipeline-cicd
- task: AzurePowerShell@4
  displayName: 'Create ADFv2 instance with MI'
  inputs:
    azureSubscription: $(AzureServiceConnectionId)
    ScriptType: InlineScript
    Inline: "Set-AzDataFactoryV2 -ResourceGroupName $(RG) -Location $(LOC) -Name $(ADFV2) -Force"
    azurePowerShellVersion: LatestVersion
- task: AzureCLI@1
  displayName: 'Create resources'
  inputs:
    azureSubscription: $(AzureServiceConnectionId)
    scriptType: bash
    scriptPath: '../blog-datapipeline-cicd/scripts/1_create_resources.sh'
- task: AzureCLI@1
  displayName: 'Configure Databricks'
  inputs:
    azureSubscription: $(AzureServiceConnectionId)
    scriptType: bash
    scriptPath: "../blog-datapipeline-cicd/scripts/2_configure_databricks.sh"
- task: AzureCLI@1
  displayName: 'Configure access to storage account'
  inputs:
    azureSubscription: $(AzureServiceConnectionId)
    scriptType: bash
    scriptPath: '../blog-datapipeline-cicd/scripts/3_configure_access_storage_databricks.sh'
- task: AzureCLI@1
  displayName: 'Configure Secret Scope Databricks'
  inputs:
    azureSubscription: $(AzureServiceConnectionId)
    scriptType: bash
    scriptPath: '../blog-datapipeline-cicd/scripts/4_configure_secret_scope_databricks.sh'
- task: AzureCLI@1
  displayName: 'Configure Mounting to Databricks'
  inputs:
    azureSubscription: $(AzureServiceConnectionId)
    scriptType: bash
    scriptPath: '../blog-datapipeline-cicd/scripts/5_configure_mount_storage_databricks.sh'
- task: AzureCLI@1
  displayName: 'Configure Firewall'
  inputs:
    azureSubscription: $(AzureServiceConnectionId)
    scriptType: bash
    scriptPath: '../blog-datapipeline-cicd/scripts/6_configure_firewall.sh'
- checkout: blog-datapipeline-deployadfv2
  path: blog-datapipeline-deployadfv2
- task: AzureResourceManagerTemplateDeployment@3
  displayName: 'Deploy ARM template ADFv2'
  inputs:
    azureResourceManagerConnection: $(AzureServiceConnectionId)
    subscriptionId: $(SUB)
    resourceGroupName: $(RG)
    location: $(LOC)
    csmFile: '../blog-datapipeline-deployadfv2/blog-datapipeline-adfv2/ARMTemplateForFactory.json'
    csmParametersFile: '../blog-datapipeline-deployadfv2/blog-datapipeline-adfv2/ARMTemplateParametersForFactory.json'
    overrideParameters: "-factoryName $(ADFV2) -dataFactory_properties_globalParameters_akv_url_value $(akv_url) -dataFactory_properties_globalParameters_stor_url_value $(stor_url) -dataFactory_properties_globalParameters_stor_name_value $(STOR) -dataFactory_properties_globalParameters_cosmosdb_name_value $(COSMOSDBNAME) -dataFactory_properties_globalParameters_dbr_resource_id_value $(dbr_resource_id) -dataFactory_properties_globalParameters_workspace_id_url_value $(workspace_id_url) -dataFactory_properties_globalParameters_cluster_id_value $(cluster_id) -dataFactory_properties_globalParameters_vaultBaseUrl_value $(akv_url) -dataFactory_location $(LOC) -dataFactory_properties_globalParameters_notebook_name_value /insert_data_CosmosDB_Gremlin.py"
- task: AzurePowerShell@4
  displayName: 'Run ADFv2 pipeline - standard'
  inputs:
    azureSubscription: $(AzureServiceConnectionId)
    ScriptType: InlineScript
    Inline: "Invoke-AzDataFactoryV2Pipeline -ResourceGroupName $(RG) -DataFactoryName $(ADFV2) -PipelineName \"blog-datapipeline-pipeline-dbrmi\""
    azurePowerShellVersion: LatestVersion